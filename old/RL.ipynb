{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains Curiosity and Skills wrapper implemented for a semi-supervised reinforcement learning built on [stable baseline3](https://github.com/DLR-RM/stable-baselines3) [example](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/stable_baselines_getting_started.ipynb)"
      ],
      "metadata": {
        "id": "04CXv2y_ADfW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twSRtd9_2jxk",
        "outputId": "8f6a294f-8a2b-4487-b684-f315a56ca545"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.4.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting gymnasium<1.1.0,>=0.29.1 (from stable-baselines3[extra])\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.8.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.10.0.84)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.17.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.6)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.9.4)\n",
            "Collecting ale-py>=0.9.0 (from stable-baselines3[extra])\n",
            "  Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (11.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py>=0.9.0->stable-baselines3[extra]) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<1.1.0,>=0.29.1->stable-baselines3[extra])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.68.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n",
            "Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stable_baselines3-2.4.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium, ale-py, stable-baselines3\n",
            "Successfully installed ale-py-0.10.1 farama-notifications-0.0.4 gymnasium-1.0.0 stable-baselines3-2.4.0\n"
          ]
        }
      ],
      "source": [
        "pip install stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stable_baselines3\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.ppo import MlpPolicy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecEnvWrapper\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ],
      "metadata": {
        "id": "duh16RCz2nXh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "model = PPO(MlpPolicy, env, verbose=0)"
      ],
      "metadata": {
        "id": "Jci_rKJk_GDA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a separate environement for evaluation\n",
        "eval_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "# Random Agent, before training\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqtkBgkd_Llk",
        "outputId": "0ea76654-a1ac-46f7-a14a-b95f73a6f81b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward:8.80 +/- 0.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the agent for 10000 steps\n",
        "model.learn(total_timesteps=10_000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCzmZRc5_N6G",
        "outputId": "ff240930-c313-493b-9d23-17fc6b23d128"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7dbdb2e46c80>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the trained agent\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhvOE_8A_lyX",
        "outputId": "44d9926f-9e65-47f1-a1c3-bc66da7dc5ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward:327.62 +/- 133.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ForwardModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple forward dynamics model: predicts next state features from current state and action.\n",
        "    For demonstration, we'll assume a simple MLP that takes state and action as input.\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
        "        super(ForwardModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, state_dim)  # Predict next state representation\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], dim=-1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)"
      ],
      "metadata": {
        "id": "qFRTTBZl3Dkv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CuriosityEnvWrapper(VecEnvWrapper):\n",
        "    \"\"\"\n",
        "    VecEnv wrapper that adds curiosity-driven intrinsic rewards.\n",
        "    \"\"\"\n",
        "    def __init__(self, venv, state_dim, action_dim, learning_rate=1e-3):\n",
        "        super().__init__(venv)\n",
        "        self.forward_model = ForwardModel(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.forward_model.parameters(), lr=learning_rate)\n",
        "\n",
        "        # store previous states and actions to train the model\n",
        "        self.last_obs = None\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.venv.reset()\n",
        "        self.last_obs = obs\n",
        "        return obs\n",
        "\n",
        "    def step_wait(self):\n",
        "        obs, rewards, dones, infos = self.venv.step_wait()\n",
        "\n",
        "        # convert to tensors\n",
        "        obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
        "        last_obs_tensor = torch.tensor(self.last_obs, dtype=torch.float32)\n",
        "\n",
        "        # override the step_async and store the actions.\n",
        "        actions_one_hot = torch.zeros((len(self.last_actions), self.action_space.n))\n",
        "        for i, a in enumerate(self.last_actions):\n",
        "            actions_one_hot[i, a] = 1.0\n",
        "\n",
        "        pred_next_state = self.forward_model(last_obs_tensor, actions_one_hot)\n",
        "        intrinsic_reward = torch.mean((pred_next_state - obs_tensor)**2, dim=-1).detach().numpy()\n",
        "\n",
        "        # combine intrinsic reward with extrinsic reward\n",
        "        total_reward = rewards + intrinsic_reward\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = torch.mean((pred_next_state - obs_tensor)**2)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # update last_obs\n",
        "        self.last_obs = obs\n",
        "        return obs, total_reward, dones, infos\n",
        "\n",
        "    def step_async(self, actions):\n",
        "        # store actions for later use\n",
        "        self.last_actions = actions\n",
        "        self.venv.step_async(actions)"
      ],
      "metadata": {
        "id": "OkmtYyn03Gie"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the base environment\n",
        "env_id = \"CartPole-v1\"\n",
        "base_env = gym.make(env_id)\n",
        "\n",
        "obs_dim = base_env.observation_space.shape[0]\n",
        "act_dim = base_env.action_space.n\n",
        "base_env.close()\n",
        "\n",
        "# Create a vectorized environment\n",
        "def make_env():\n",
        "    return gym.make(env_id)\n",
        "\n",
        "venv = DummyVecEnv([make_env])\n",
        "\n",
        "# Wrap with curiosity:\n",
        "curiosity_venv = CuriosityEnvWrapper(venv, state_dim=obs_dim, action_dim=act_dim)"
      ],
      "metadata": {
        "id": "jHU5qeSjDFKV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PPO(\"MlpPolicy\", curiosity_venv, verbose=1, n_steps=2048, batch_size=64, ent_coef=0.0, learning_rate=3e-4, n_epochs=10)\n",
        "eval_env = gym.make(\"CartPole-v1\")\n",
        "# Random Agent, before training\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74OQOSO4DFD5",
        "outputId": "dd7122a4-0a15-42a8-f80a-3681f42bb917"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward:29.28 +/- 9.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.learn(total_timesteps=20000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1AciaS5HDTA",
        "outputId": "ad26bdec-a6b0-42b3-ae54-6d43b6edd063"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 391  |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 5    |\n",
            "|    total_timesteps | 2048 |\n",
            "-----------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 315         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 12          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008498171 |\n",
            "|    clip_fraction        | 0.0916      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.686      |\n",
            "|    explained_variance   | -0.00794    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.74        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0137     |\n",
            "|    value_loss           | 61.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 317         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 19          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009447199 |\n",
            "|    clip_fraction        | 0.0735      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.671      |\n",
            "|    explained_variance   | 0.0612      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 12.2        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0186     |\n",
            "|    value_loss           | 36.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 309         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 26          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012101745 |\n",
            "|    clip_fraction        | 0.122       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.634      |\n",
            "|    explained_variance   | 0.27        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 21.2        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0242     |\n",
            "|    value_loss           | 50.5        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 311        |\n",
            "|    iterations           | 5          |\n",
            "|    time_elapsed         | 32         |\n",
            "|    total_timesteps      | 10240      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00744828 |\n",
            "|    clip_fraction        | 0.0705     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.611     |\n",
            "|    explained_variance   | 0.249      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 20.2       |\n",
            "|    n_updates            | 40         |\n",
            "|    policy_gradient_loss | -0.0178    |\n",
            "|    value_loss           | 59.8       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 308         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 39          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010298828 |\n",
            "|    clip_fraction        | 0.0917      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.593      |\n",
            "|    explained_variance   | 0.497       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 19.3        |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0145     |\n",
            "|    value_loss           | 50.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 309         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 46          |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007545392 |\n",
            "|    clip_fraction        | 0.0411      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.594      |\n",
            "|    explained_variance   | 0.718       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.52        |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.00791    |\n",
            "|    value_loss           | 44.5        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 306          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 53           |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052044373 |\n",
            "|    clip_fraction        | 0.057        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.58        |\n",
            "|    explained_variance   | 0.793        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 20.8         |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.00739     |\n",
            "|    value_loss           | 37.1         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 307         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 59          |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008182382 |\n",
            "|    clip_fraction        | 0.0805      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.571      |\n",
            "|    explained_variance   | 0.699       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.49        |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0108     |\n",
            "|    value_loss           | 53.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 307         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 66          |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010097038 |\n",
            "|    clip_fraction        | 0.0854      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.568      |\n",
            "|    explained_variance   | 0.884       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.08        |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.00817    |\n",
            "|    value_loss           | 24.5        |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7ba443f1cf40>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_env = gym.make(\"CartPole-v1\")\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IANDNbIWEYDV",
        "outputId": "81a04ff5-3b89-415c-e028-262f95fdcb5e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward:499.43 +/- 3.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Skill\n"
      ],
      "metadata": {
        "id": "wxe3cjKEdHXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkillWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    A skill wrapper that augments observations with a skill embedding.\n",
        "    Each episode, a skill index is sampled and a one-hot skill vector is appended to the observation.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, skill_dim=4):\n",
        "        super().__init__(env)\n",
        "        self.skill_dim = skill_dim\n",
        "        self.current_skill = None\n",
        "\n",
        "        orig_obs_space = self.env.observation_space\n",
        "\n",
        "        # assume original obs space is a Box\n",
        "        low = np.concatenate([orig_obs_space.low, np.zeros(self.skill_dim)])\n",
        "        high = np.concatenate([orig_obs_space.high, np.ones(self.skill_dim)])\n",
        "        self.observation_space = gym.spaces.Box(low=low, high=high, dtype=orig_obs_space.dtype)\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        self.current_skill = np.random.randint(self.skill_dim)\n",
        "        obs = self._augment_obs(obs, self.current_skill)\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, truncated, info = self.env.step(action)\n",
        "        obs = self._augment_obs(obs, self.current_skill)\n",
        "        return obs, reward, done, truncated, info\n",
        "\n",
        "    def _augment_obs(self, obs, skill_idx):\n",
        "        skill_vec = np.zeros(self.skill_dim, dtype=obs.dtype)\n",
        "        skill_vec[skill_idx] = 1.0\n",
        "        return np.concatenate([obs, skill_vec])\n",
        "\n",
        "\n",
        "def make_env_skill(env_id=\"CartPole-v1\", skill_dim=4):\n",
        "    def _init():\n",
        "        env = gym.make(env_id)\n",
        "        env = SkillWrapper(env, skill_dim=skill_dim)\n",
        "        return env\n",
        "    return _init"
      ],
      "metadata": {
        "id": "xlRju2wHEeaj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_id = \"CartPole-v1\"\n",
        "skill_dim = 4\n",
        "venv = DummyVecEnv([make_env_skill(env_id, skill_dim) for _ in range(1)])\n",
        "\n",
        "sample_env = gym.make(env_id)\n",
        "obs_dim = sample_env.observation_space.shape[0]\n",
        "sample_env.close()\n",
        "# combine dimensions\n",
        "aug_obs_dim = obs_dim + skill_dim\n",
        "act_dim = 2  # CartPole has 2 discrete actions\n",
        "\n",
        "# wrap with curiosity\n",
        "curiosity_venv = CuriosityEnvWrapper(venv, state_dim=aug_obs_dim, action_dim=act_dim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALOys8ZkdG4p",
        "outputId": "d82cd024-264e-4807-8b28-f57ee0799f9d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = PPO(\"MlpPolicy\", curiosity_venv, verbose=1, n_steps=2048, batch_size=64, ent_coef=0.0, learning_rate=3e-4, n_epochs=10)\n",
        "eval_env = SkillWrapper(gym.make(\"CartPole-v1\"), skill_dim=4)\n",
        "# Random Agent, before training\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Kg8m8TPdG2M",
        "outputId": "e63e1afa-4721-4651-b99a-65d2c87980da"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward:39.52 +/- 56.39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.learn(total_timesteps=20000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwD6PdR1dGzx",
        "outputId": "cb73a4db-ca1b-4933-fadd-948d745bb966"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 400  |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 5    |\n",
            "|    total_timesteps | 2048 |\n",
            "-----------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 335        |\n",
            "|    iterations           | 2          |\n",
            "|    time_elapsed         | 12         |\n",
            "|    total_timesteps      | 4096       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00980261 |\n",
            "|    clip_fraction        | 0.104      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.686     |\n",
            "|    explained_variance   | -0.0025    |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 5.74       |\n",
            "|    n_updates            | 10         |\n",
            "|    policy_gradient_loss | -0.0152    |\n",
            "|    value_loss           | 39.6       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 323         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 18          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009025866 |\n",
            "|    clip_fraction        | 0.049       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.672      |\n",
            "|    explained_variance   | 0.102       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 12.2        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0143     |\n",
            "|    value_loss           | 35.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 306         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 26          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007982082 |\n",
            "|    clip_fraction        | 0.0798      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.639      |\n",
            "|    explained_variance   | 0.194       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 24.9        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0165     |\n",
            "|    value_loss           | 49.9        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 301          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 33           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0060931123 |\n",
            "|    clip_fraction        | 0.057        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.615       |\n",
            "|    explained_variance   | 0.234        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 28.8         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.0147      |\n",
            "|    value_loss           | 67           |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 304         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 40          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004339158 |\n",
            "|    clip_fraction        | 0.0338      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.607      |\n",
            "|    explained_variance   | 0.165       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 25.5        |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.012      |\n",
            "|    value_loss           | 71          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 302         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 47          |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006184442 |\n",
            "|    clip_fraction        | 0.0399      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.601      |\n",
            "|    explained_variance   | 0.42        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 18          |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.00692    |\n",
            "|    value_loss           | 71.8        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 304          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 53           |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0068251947 |\n",
            "|    clip_fraction        | 0.0662       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.582       |\n",
            "|    explained_variance   | 0.495        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 24.5         |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.0114      |\n",
            "|    value_loss           | 65.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 301          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 61           |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0049420875 |\n",
            "|    clip_fraction        | 0.0333       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.584       |\n",
            "|    explained_variance   | 0.454        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 10.5         |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.00586     |\n",
            "|    value_loss           | 52.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 303          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 67           |\n",
            "|    total_timesteps      | 20480        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0056793774 |\n",
            "|    clip_fraction        | 0.0718       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.575       |\n",
            "|    explained_variance   | 0.883        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.44         |\n",
            "|    n_updates            | 90           |\n",
            "|    policy_gradient_loss | -0.0112      |\n",
            "|    value_loss           | 31.7         |\n",
            "------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7ba440d46260>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_env = gym.make(\"CartPole-v1\")\n",
        "eval_env = SkillWrapper(eval_env, skill_dim=4)\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGfN4Wq8esTS",
        "outputId": "5bfcf750-69b9-414b-f052-2f68de3c5bba"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward:431.59 +/- 79.92\n"
          ]
        }
      ]
    }
  ]
}